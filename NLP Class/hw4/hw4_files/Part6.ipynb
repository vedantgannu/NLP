{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b49aa4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForMaskedLM: ['activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "from lexsub_xml import read_lexsub_xml\n",
    "from lexsub_xml import Context \n",
    "\n",
    "# suggested imports \n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "\n",
    "#tensorflow.compat.v1.disable_eager_execution()\n",
    "# import logging\n",
    "# logging.disable(logging.INFO)\n",
    "\n",
    "import gensim\n",
    "import transformers\n",
    "\n",
    "#from transformers.utils import hf_logging\n",
    "#hf_logging.disable_progress_bar()\n",
    "#transformers.utils.logging.set_verbosity(transformers.logging.ERROR)\n",
    "# import evaluate\n",
    "# evaluate.logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "\n",
    "from typing import List\n",
    "import string\n",
    "\n",
    "def tokenize(s): \n",
    "    \"\"\"\n",
    "    a naive tokenizer that splits on punctuation and whitespaces.  \n",
    "    \"\"\"\n",
    "    s = \"\".join(\" \" if x in string.punctuation else x for x in s.lower())    \n",
    "    return s.split() \n",
    "\n",
    "def get_candidates(lemma, pos) -> List[str]:\n",
    "    # Part 1\n",
    "    ret = set()\n",
    "    for synset in wn.synsets(lemma, pos=pos):\n",
    "        for lexeme in synset.lemmas():\n",
    "            if lexeme.name() == lemma:\n",
    "                continue\n",
    "            elif lexeme.name().find(\"_\") != -1:\n",
    "                ret.add(lexeme.name().replace(\"_\", \" \"))\n",
    "            else:\n",
    "                ret.add(lexeme.name())\n",
    "    return list(ret)\n",
    "\n",
    "def smurf_predictor(context : Context) -> str:\n",
    "    \"\"\"\n",
    "    suggest 'smurf' as a substitute for all words.\n",
    "    \"\"\"\n",
    "    return 'smurf'\n",
    "\n",
    "def wn_frequency_predictor(context : Context) -> str:\n",
    "    #return None # replace for part 2\n",
    "    #Context obj: self.cid, self.word_form, self.lemma, self.pos, self.left_context, self.right_context\n",
    "    synsets = wn.synsets(context.lemma, pos=context.pos)#Get the synonym set that the input word relates to\n",
    "    frequency = defaultdict(int)\n",
    "    for synset in synsets:\n",
    "        for lexeme in synset.lemmas():\n",
    "            if lexeme.name() != context.lemma:#Consider the lemmas that aren't the input lemma (synonyms)\n",
    "                frequency[lexeme.name()] += lexeme.count()#Record the number of occurences for each synonym sharing word sense with input lemma\n",
    "                \n",
    "    return max(frequency, key=frequency.get).replace(\"_\", \" \")\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "def wn_simple_lesk_predictor(context : Context) -> str:\n",
    "    synsets = wn.synsets(context.lemma, pos=context.pos)#Get the synonym set that the input word relates to\n",
    "    stop_words = stopwords.words('english')\n",
    "    max_overlap = 0\n",
    "    overlap_dict = defaultdict(int)\n",
    "    for synset in synsets:\n",
    "        #Tokenize and filter out stop words of synset definition, left context, right context, and examples\n",
    "        definitions = [ [word.lower() for word in tokenize(synset.definition()) if word.lower() not in stop_words] ]\n",
    "        left_context = tokenize(\" \".join([word.lower() for word in context.left_context if word.lower() not in stop_words]))\n",
    "        right_context = tokenize(\" \".join([word.lower() for word in context.right_context if word.lower() not in stop_words]))\n",
    "        examples = []\n",
    "        for example in synset.examples():\n",
    "            examples.append([word.lower() for word in tokenize(example) if word.lower() not in stop_words])\n",
    "        #Do same filtering and tokenization for hypernym definitions and examples\n",
    "        for synset_hyper in synset.hypernyms():\n",
    "            definitions.append( [word.lower() for word in tokenize(synset_hyper.definition()) if word.lower() not in stop_words] )\n",
    "            for example in synset_hyper.examples():\n",
    "                examples.append([word.lower() for word in tokenize(example) if word.lower() not in stop_words])\n",
    "        #Get the overlap\n",
    "        overlap = 0\n",
    "        for gloss in definitions + examples:\n",
    "            overlap += len(set(gloss) & set(left_context)) + len(set(gloss) & set(right_context))\n",
    "        # if overlap > 0:\n",
    "        #     overlap_dict[synset] = overlap\n",
    "        overlap_dict[synset] = overlap\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "    #best_synsets = [synset for (synset, overlap) in overlap_dict.items() if overlap == max_overlap]\n",
    "    best_synsets = [synset for (synset, overlap) in overlap_dict.items() if overlap == max_overlap and overlap != 0]\n",
    "    \n",
    "    lexemes = []\n",
    "    for synset in (best_synsets if best_synsets else synsets):\n",
    "        for lexeme in synset.lemmas():\n",
    "            if lexeme.name() != context.lemma:\n",
    "                lexemes.append(lexeme)\n",
    "    if lexemes:\n",
    "        return max(lexemes, key=lambda x: x.count()).name().replace(\"_\", \" \")\n",
    "    else:\n",
    "        return \"smurf\"\n",
    "                \n",
    "            \n",
    "    #return None #replace for part 3\n",
    "   \n",
    "\n",
    "class Word2VecSubst(object):\n",
    "        \n",
    "    def __init__(self, filename):\n",
    "        self.model = gensim.models.KeyedVectors.load_word2vec_format(filename, binary=True)    \n",
    "\n",
    "    def predict_nearest(self,context : Context) -> str:\n",
    "        syns = get_candidates(context.lemma, context.pos)\n",
    "        lemmas = []\n",
    "        for syn in syns:\n",
    "            try:\n",
    "                lemmas.append((syn, self.model.similarity(context.lemma, syn.replace(\" \",  \"_\"))))\n",
    "            except:\n",
    "                continue\n",
    "        if lemmas:\n",
    "             return max(lemmas, key=lambda lemma: lemma[1])[0]\n",
    "        else:\n",
    "            return \"smurf\"\n",
    "        #return None # replace for part 4\n",
    "\n",
    "\n",
    "class BertPredictor(object):\n",
    "\n",
    "    def __init__(self): \n",
    "        self.tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.model = transformers.TFDistilBertForMaskedLM.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    def predict(self, context : Context) -> str:\n",
    "        syns = get_candidates(context.lemma, context.pos)\n",
    "        left_context = ''\n",
    "        for word in context.left_context:\n",
    "            if word.isalpha():\n",
    "                left_context = left_context + ' ' + word\n",
    "            else:#Handle things like \"do n't -> don't\"    ;    Dubhghall , from -> Dubhghall, from\n",
    "                left_context = left_context + word\n",
    "\n",
    "        sentence = left_context + ' [MASK]'\n",
    "\n",
    "        right_context = ''\n",
    "        for word in context.right_context:\n",
    "            if word.isalpha():\n",
    "                right_context = right_context + ' ' + word\n",
    "            else:#Handle things like \"do n't -> don't\"    ;    Dubhghall , from -> Dubhghall, from\n",
    "                right_context = right_context + word\n",
    "        \n",
    "        sentence = sentence + right_context\n",
    "\n",
    "        input_toks_encoded = self.tokenizer.encode(sentence)\n",
    "        mask_index = self.tokenizer.convert_ids_to_tokens(input_toks_encoded).index('[MASK]')\n",
    "        input_mat = np.array(input_toks_encoded).reshape((1,-1))\n",
    "        outputs = self.model.predict(input_mat, verbose=0)\n",
    "        predictions = outputs[0]\n",
    "        best_words_indices = np.argsort(predictions[0][mask_index])[::-1] # Sort in increasing order\n",
    "        best_words = self.tokenizer.convert_ids_to_tokens(best_words_indices)\n",
    "        for word in best_words:\n",
    "            if word.replace(\"_\", \" \") in syns:\n",
    "                return word.replace(\"_\", \" \")\n",
    "        return \"\"\n",
    "        #return None # replace for part 5\n",
    "        \n",
    "\n",
    "def part3(context : Context) -> str:\n",
    "    synsets = wn.synsets(context.lemma, pos=context.pos)#Get the synonym set that the input word relates to\n",
    "    stop_words = stopwords.words('english')\n",
    "    max_overlap = 0\n",
    "    overlap_dict = defaultdict(int)\n",
    "    for synset in synsets:\n",
    "        #Tokenize and filter out stop words of synset definition, left context, right context, and examples\n",
    "        definitions = [ [word.lower() for word in tokenize(synset.definition()) if word.lower() not in stop_words] ]\n",
    "        left_context = tokenize(\" \".join([word.lower() for word in context.left_context if word.lower() not in stop_words]))\n",
    "        right_context = tokenize(\" \".join([word.lower() for word in context.right_context if word.lower() not in stop_words]))\n",
    "        examples = []\n",
    "        for example in synset.examples():\n",
    "            examples.append([word.lower() for word in tokenize(example) if word.lower() not in stop_words])\n",
    "        #Do same filtering and tokenization for hypernym definitions and examples\n",
    "        for synset_hyper in synset.hypernyms():\n",
    "            definitions.append( [word.lower() for word in tokenize(synset_hyper.definition()) if word.lower() not in stop_words] )\n",
    "            for example in synset_hyper.examples():\n",
    "                examples.append([word.lower() for word in tokenize(example) if word.lower() not in stop_words])\n",
    "        #Get the overlap\n",
    "        overlap = 0\n",
    "        for gloss in definitions + examples:\n",
    "            overlap += len(set(gloss) & set(left_context)) + len(set(gloss) & set(right_context))\n",
    "        overlap_dict[synset] = overlap\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "    #best_synsets = [synset for (synset, overlap) in overlap_dict.items() if overlap == max_overlap and overlap != 0]\n",
    "    best_synsets = [synset for (synset, overlap) in overlap_dict.items() if overlap == max_overlap]\n",
    "    lexemes = []\n",
    "    best_synset_frequency = 0\n",
    "    # lexeme_frequency_dictionary = defaultdict(int)\n",
    "    # synset_dict = defaultdict(int)\n",
    "    # for synset in (best_synsets if best_synsets else synsets):#Should handle if overlap/no overlap exists\n",
    "    #     synset_dict[synset] = 0\n",
    "    #     for lexeme in synset.lemmas():\n",
    "    #         if lexeme.name() != context.lemma:#Consider the lemmas that aren't the input lemma (synonyms)\n",
    "    #             lexeme_frequency_dictionary[lexeme.name()] += lexeme.count()#Record the number of occurences for each synonym sharing word sense with input lemma \n",
    "    #Get the most frequent synset(s) (synset that has the highest frequency counts of lexemes)\n",
    "    synset_dict = defaultdict(int)\n",
    "    #print(best_synsets)\n",
    "    for synset in (best_synsets if best_synsets else synsets):#Should handle if overlap/no overlap exists\n",
    "        frequency = sum([lexeme.count() for lexeme in synset.lemmas() if lexeme.name() != context.lemma])\n",
    "        synset_dict[synset] = frequency\n",
    "        if frequency > best_synset_frequency:\n",
    "            best_synset_frequency = frequency\n",
    "    most_frequent_synsets = [synset for synset, freq in synset_dict.items() if freq == best_synset_frequency]\n",
    "    #Select most frequent lexeme from synset(s)\n",
    "    if most_frequent_synsets:\n",
    "        return max(itertools.chain(*[synset.lemmas() for synset in most_frequent_synsets]), key=lambda lexeme: lexeme.count()).name().replace(\"_\", \" \")\n",
    "    else:\n",
    "        return \"smurf\"\n",
    "    \n",
    "    # for lexeme in synset.lemmas():\n",
    "    #     if lexeme.name() != context.lemma:\n",
    "    #         lexemes.append(lexeme)\n",
    "    # if lexemes:\n",
    "    #     return max(lexemes, key=lambda x: x.count()).name().replace(\"_\", \" \")\n",
    "    # else:\n",
    "    #     return \"smurf\"\n",
    "    \n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = transformers.TFDistilBertForMaskedLM.from_pretrained('distilbert-base-uncased')\n",
    "wv_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e15e1b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(context : Context) -> str:\n",
    "    global tokenizer\n",
    "    global model\n",
    "    \n",
    "    syns = get_candidates(context.lemma, context.pos)\n",
    "    left_context = ''\n",
    "    for word in context.left_context:\n",
    "        if word.isalpha():\n",
    "            left_context = left_context + ' ' + word\n",
    "        else:#Handle things like \"do n't -> don't\"    ;    Dubhghall , from -> Dubhghall, from\n",
    "            left_context = left_context + word\n",
    "\n",
    "    sentence = left_context + ' [MASK]'\n",
    "\n",
    "    right_context = ''\n",
    "    for word in context.right_context:\n",
    "        if word.isalpha():\n",
    "            right_context = right_context + ' ' + word\n",
    "        else:#Handle things like \"do n't -> don't\"    ;    Dubhghall , from -> Dubhghall, from\n",
    "            right_context = right_context + word\n",
    "\n",
    "    sentence = sentence + right_context\n",
    "    \n",
    "    #Input CLS sentence SEP sentence SEP to BERT\n",
    "    input_toks_encoded = tokenizer.encode(left_context + right_context) + tokenizer.encode(sentence)[1:]\n",
    "    mask_index = tokenizer.convert_ids_to_tokens(input_toks_encoded).index('[MASK]')\n",
    "    input_mat = np.array(input_toks_encoded).reshape((1,-1))\n",
    "    outputs = model.predict(input_mat, verbose=0)\n",
    "    predictions = outputs[0]\n",
    "    best_words_indices = np.argsort(predictions[0][mask_index])[::-1] # Sort in increasing order\n",
    "    best_words = tokenizer.convert_ids_to_tokens(best_words_indices)\n",
    "    \n",
    "    best_word = \"\"\n",
    "    scores = []\n",
    "    i = 0\n",
    "    for word in best_words:\n",
    "        if i > 10:\n",
    "            break\n",
    "        #print(i, word)\n",
    "        word_ = best_words[i]\n",
    "        #word_ = word.replace(\"_\", \" \")\n",
    "        if word_.replace(\"_\", \" \") != context.lemma:\n",
    "            score = 0\n",
    "            for syn in syns:\n",
    "                try:\n",
    "                    score += wv_model.similarity(word_, syn.replace(\" \",  \"_\"))\n",
    "                except:\n",
    "                    continue\n",
    "            scores.append((word_, score/len(syns)))\n",
    "            i += 1\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    if scores:\n",
    "        return max(scores)[0].replace(\"_\", \" \")\n",
    "    else:\n",
    "        return \"smurf\"\n",
    "            \n",
    "                    \n",
    "            \n",
    "#         if word.replace(\"_\", \" \") in syns and word.replace(\"_\", \" \") != context.lemma:\n",
    "#             return word.replace(\"_\", \" \")\n",
    "    \n",
    "    #return \"\"\n",
    "    #return None # replace for part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "837132e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "younger\n",
      "thinner\n",
      "tiny\n",
      "strong\n",
      "worst\n",
      "youngest\n",
      "true\n",
      "deep\n",
      "wide\n",
      "wrong\n",
      "productions\n",
      "smurf\n",
      "reality\n",
      "smurf\n",
      "music\n",
      "smurf\n",
      "serials\n",
      "smurf\n",
      "project\n",
      "ventures\n",
      "your\n",
      "weigh\n",
      "smurf\n",
      "took\n",
      "ignore\n",
      "have\n",
      "unfolded\n",
      "get\n",
      "smurf\n",
      "took\n",
      "wet\n",
      "grown\n",
      "were\n",
      "yours\n",
      "smaller\n",
      "smurf\n",
      "warm\n",
      "too\n",
      "up\n",
      "open\n",
      "quality\n",
      "smurf\n",
      "sky\n",
      "system\n",
      "powder\n",
      "variable\n",
      "waist\n",
      "smurf\n",
      "rooms\n",
      "survive\n",
      "product\n",
      "v\n",
      "smurf\n",
      "hybrid\n",
      "truth\n",
      "whale\n",
      "smurf\n",
      "test\n",
      "racial\n",
      "used\n",
      "was\n",
      "）\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\uff09' in position 16: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m predict(context)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(prediction)\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m :: \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#print(\"{}.{} {} :: {}\".format(context.lemma, context.pos, context.cid, prediction))\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#!perl score.pl smurf6.predict gold.trial\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\nlpclass\\lib\\encodings\\cp1252.py:19\u001b[0m, in \u001b[0;36mIncrementalEncoder.encode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\uff09' in position 16: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "with open('smurf6.predict', 'w') as f:\n",
    "  for context in read_lexsub_xml(\"lexsub_trial.xml\"):\n",
    "    #print(context)  # useful for debugging\n",
    "    prediction = predict(context)\n",
    "    print(prediction)\n",
    "    print(\"{}.{} {} :: {}\".format(context.lemma, context.pos, context.cid, prediction), file=f)\n",
    "    #print(\"{}.{} {} :: {}\".format(context.lemma, context.pos, context.cid, prediction))\n",
    "#!perl score.pl smurf6.predict gold.trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b41215a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total = 298, attempted = 298\n",
      "precision = 0.117, recall = 0.117\n",
      "Total with mode 206 attempted 206\n",
      "precision = 0.175, recall = 0.175\n"
     ]
    }
   ],
   "source": [
    "!perl score.pl smurf5.predict gold.trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d135b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
